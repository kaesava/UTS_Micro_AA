---
title: "Assignment Report - Microcredentials UTS Advanced Analytics"
author: "Kalyanaraman (Kesh) Kshetrapalapuram (Telstra)"
header-includes:
  - \usepackage{enumitem}
  - \setlistdepth{20}
  - \renewlist{itemize}{itemize}{20}
  - \renewlist{enumerate}{enumerate}{20}
output:
  word_document: default
  pdf_document: 
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, echo = FALSE, warning=FALSE, error=FALSE, message=FALSE)
# add eval = FALSE, when knitting finally
```

# Introduction

## The Problem, Inputs & Outputs

The assignment task was to use the provided training dataset to build a robust classifier (classification model) that can accurately predict the `QUALIFIED` target attribute (binary `0` meaning not qualified or `1` meaning qualified) on unseen data. The ask was to then use the model to predict the `QUALIFIED` target attribute for the provided test dataset, and write a report summarising the approach, results, and a discussion on learnings.

From the descriptions of the attributes provided, it seems the dataset represents property data, where we are likely attempting to understand when a potential sale is qualified.

### Input

A training dataset with examples (rows/records) and attributes (columns / features), along with the target attribute (in this case `QUALIFIED`).

The training (`Assignment-HousingDataset.csv`) and testing (`Assignment-UnknownDataset.csv`) csv files were downloaded from the UTS Canvas site (`https://canvas.open.uts.edu.au/courses/973/assignments`).

### Outputs
* The "best" classification model that can be used to predict the `QUALIFIED` attribute
* The prediction (`QUALIFIED` attribute) for the provided test examples
* This report outlining approach, configuration, results and learnings (this report)

While this report does not show source `R` code, the full code is available from here: https://github.com/kaesava/UTS_Micro_AA/blob/main/assignment.Rmd. An exploration `R` file is also available here: https://github.com/kaesava/UTS_Micro_AA/blob/main/explore.R

# High-level Approach

I decided to use `R`, so I can learn another programming language, one that seems to be popular among data scientists. I installed the latest versions of `R` (v 4.0.3) and `R Studio` (v1.3.1093), and wrote this assignment as an `R` Notebook that can be easily compiled into Word and/or PDF.

A a high level, I followed these steps:

* I prepared the `R` environment by loading the packages I needed.
* I loaded the provided training and testing datasets and explored the data using summary statistics and visualisations to learn more about each attribute in the training dataset (missing values, distribution, distribution relative to target attribute, etc.).
* I split the training data into training (90%) and validation (10%), so that performance measures are reported on new (unseen) data.
* I pre-processed the data by removing redundant attributes, imputing missing values,  identifying and removing highly correlated numeric and categorical attributes, testing for outliers, creating new calculated attributes (like date durations),  one-hot-encoding categorical attributes and normalising/binning numeric attributes as needed. I found that this step was highly iterative. I built a "reference" random forest model (with default hyper-parameters), and depending on the accuracy (f-score) of the resulting classifier, I re-visited and tuned each pre-processing step several times.
* I ran feature/attribute selection algorithms to determine importance and removed attributes that didn't contribute to the classification.
* I balanced the training dataset so that we had the same number of examples in the `0` and `1` target attributes
* I setup the training (classification) model parameters (like 10-fold cross validation). The random seed for every run of every training algorithm was set to a static number (`3433`) to minimise uncertainty through randomness.
* I ran 7 different classification algorithms (decision tree, k nearest neighbour, random forest, GBM, SVM, neural network and an ensemble model). For each classifier, I experimented with various hyper-parameters (tuning) to determine the best performing one. I collected several accuracy measures (accuracy, f-score, AUC, time to run).
* I determined the best classifier based on various criteria (accuracy, time to run, etc.) and applied it to the testing dataset, submitting the results to `kaggle`.

# Summary of Results

The GBM classifier was selected. The ideal hyper-parameters used were: `n.trees` = `450`, `interaction.depth` = `10`, `shrinkage` = `0.1`, `n.minobsinnode` = `10` on full training set.

* Accuracy: `90.83%`
* f-score: `90.18%`
* AUC: `96.04%`
* Time to run: `~ 3` hours

I've outlined reasons for selecting it in the `Selecting the best model` section below.

# Data Exploration, Pre-processing and Attribute Transformation

## Environment setup 

I loaded `R` libraries that I needed: like `caret` (for streamlining model training processes), and `ggplot2`(for visualisation). ~25 packages were used.

```{r lib_setup}
if(!require(caret)){install.packages('caret')}
if(!require(ggplot2)){install.packages('ggplot2')}
if(!require(DMwR)){install.packages('DMwR')}
if(!require(gridExtra)){install.packages('gridExtra')}
if(!require(skimr)){install.packages('skimr')}
if(!require(OneR)){install.packages('OneR')}
if(!require(BBmisc)){install.packages('BBmisc')}
if(!require(Hmisc)){install.packages('Hmisc')}
if(!require(GGally)){install.packages('GGally')}
if(!require(ranger)){install.packages('ranger')}
if(!require(missRanger)){install.packages('missRanger')}
if(!require(prettyunits)){install.packages('prettyunits')}
if(!require(dummy)){install.packages('dummy')}
if(!require(Boruta)){install.packages('Boruta')}
if(!require(ROCR)){install.packages('ROCR')}
if(!require(C50)){install.packages('C50')}
if(!require(lubridate)){install.packages('lubridate')}
if(!require(GoodmanKruskal)){install.packages('GoodmanKruskal')}
if(!require(dplyr)){install.packages('dplyr')}
if(!require(MASS)){install.packages('MASS')}
if(!require(vcd)){install.packages('vcd')}
if(!require(leaps)){install.packages('leaps')}
if(!require(pROC)){install.packages('pROC')}
if(!require(gbm)){install.packages('gbm')}
if(!require(imager)){install.packages('imager')}
library(caret)
library(ggplot2)
library(DMwR)
library(gridExtra)
library(skimr)
library(OneR)
library(BBmisc)
library(Hmisc)
library(GGally)
library(ranger)
library(missRanger)
library(prettyunits)
library(dummy)
library(Boruta)
library(ROCR)
library(C50)
library(lubridate)
library(GoodmanKruskal)
library(dplyr)
library(MASS)
library(vcd)
library(leaps)
library(pROC)
library(gbm)
library(imager)
```

## Data Load

The training (`Assignment-HousingDataset.csv`) and testing (`Assignment-UnknownDataset.csv`) csv files were downloaded from the UTS Canvas site (`https://canvas.open.uts.edu.au/courses/973/assignments`) and loaded into `R`.

They were comma separated, with a header row, and had missing values left blank. I used the `read.csv` function to read in the csv files.

```{r data_load}
raw_train <- read.csv("Assignment-HousingDataset.csv", nrows=75008, header=TRUE, quote="\"", sep=",", na.strings = "")
raw_test <- read.csv("Assignment-UnknownDataset.csv", nrows=32148, header=TRUE, quote="\"", sep=",", na.strings = "")
train <- raw_train
test <- raw_test
```

The training dataset had `75,007` examples and `38` attributes (including the target attribute `QUALIFIED`). The testing dataset had `32,147` examples and `37` attributes (it did not include the target attribute). The attributes were visually examined. They included strings, dates and numbers, and had attributes with missing values.

```{r visual_check}
summ_train <- summary(raw_train)
str_train <- str(raw_train)
n_missing <- sapply(raw_train, function(x) sum(is.na(x)))
```

A brief explanation of each attribute is available from the UTS Canvas site (https://canvas.open.uts.edu.au/courses/973/assignments/2644) under the `Dataset` section.

## Data Exploration & Pre-Processing

### Using a Reference model

A reference random forest model with 10-fold cross validation was used to test whether to (and how to) cleanse and transform attributes. If performance (f-score) deteriorated after a pre-processing step, the transformation was rolled back. Default hyper-parameters and a static random seed were used to minimise variation due to randomness. The validation set was different from the training set used.

Further, all transformation applied to the training dataset were also applied to the validation and testing datasets.

### Setting the target attribute as categorical

The target `QUALIFIED` attribute was converted to a categorical attribute (known as a `factor` in `R`), as this is a classification problem (not a regression one).

```{r explore_target}
train[,"QUALIFIED"] = factor(train[,"QUALIFIED"], labels = c("no", "yes"))
```

### Dropping attributes that add no value

The following attributes were dropped:

* `Row ID`: It is a unique arbitrary row identifier and therefore should not participate in prediction 
* `GIS_LAST_MOD_DTTM`: Every example had the same value; it therefore cannot play a role in discriminating between the target attribute levels
* `HEAT`, `STYLE`, `STRUCT`, `GRADE`, `CNDTN`, `EXTWALL`, `ROOF`, and `INTWALL`: Each of these descriptive attributes are already coded for by another attribute; the 1:1 between the code and description for each of these attributes was tested before they were dropped.

```{r drop_redundant_attribs}
cols <- c("row.ID", "GIS_LAST_MOD_DTTM", "HEAT_D", "STYLE_D", "STRUCT_D", "GRADE_D", "CNDTN_D", "EXTWALL_D", "ROOF_D", "INTWALL_D")

train <- train[,!names(train) %in% cols]
test <- test[,!names(test) %in% cols]
```

### Splitting the data into Training & Validation

90% of the data was used for training (randomly sampled), and 10% kept aside for validation. This was done to ensure that when we compare the performance of different classification algorithms (or even variants of an algorithm with different hyper-parameters), it is done on a dataset never seen before, to avoid the effects of over-fitting (high variance).

```{r partition_data}
set.seed(3433)
validationRows <- createDataPartition(train$QUALIFIED, p=0.90, list=FALSE)
validation <- train[-validationRows,]
train <- train[validationRows,]
```

### Handling Date attributes

Date fields were first cleansed:

* The `SALEDATE` was read in as a string, but converted to a date. There were ~`15,885` examples with value `01/01/1900`. The initial strategy for this field was to mark these as missing (likely to be `0`) and then impute by categorising (binning) the date attribute and creating an additional `NA` category level. But this caused a reduction in both accuracy and the f-score, likely because the interpretation of `1900` as missing may not have been correct. This was therefore reverted.

* The `YR_RMDL` attribute (year or re-model) had `36,456` missing values (more than half) in the training dataset, suggesting that being missing may itself be important (it likely indicates no property re-modelling). Therefore, instead of imputing with median or mean, these were left as missing and treated (binned) as outlined in the next section. The attribute also included invalid values like `20` and so, any year that was less than `1750` (most likely errors) was set a missing.

* There were two date-year attributes (`AYB` and `EYB`) denoting year of build and improvement. They included a handful of invalid (for a year) values like `0` and `20`. These, along with missing values (less than `~10`) were imputed with the median (median was used instead of mean to eliminate the effects of extremes).

```{r clean_dates}
#train[as.Date(train$SALEDATE) == "1900-01-01","SALEDATE"] <- NA
#validation[as.Date(validation$SALEDATE) == "1900-01-01","SALEDATE"] <- NA
#test[as.Date(test$SALEDATE) == "1900-01-01","SALEDATE"] <- NA

train[,"SALEDATE"] <- as.Date(train[,"SALEDATE"])
validation[,"SALEDATE"] <- as.Date(validation[,"SALEDATE"])
test[,"SALEDATE"] <- as.Date(test[,"SALEDATE"])

train[!is.na(train$YR_RMDL) & (train$YR_RMDL < 1750),"YR_RMDL"] <- NA
validation[!is.na(validation$YR_RMDL) & (validation$YR_RMDL < 1750),"YR_RMDL"] <- NA
test[!is.na(test$YR_RMDL) & (test$YR_RMDL < 1750),"YR_RMDL"] <- NA

train[!is.na(train$YR_RMDL) & (train$YR_RMDL < 1750),"YR_RMDL"] <- median(train$YR_RMDL, na.rm = TRUE)
validation[!is.na(validation$YR_RMDL) & (validation$YR_RMDL < 1750),"YR_RMDL"] <- median(validation$YR_RMDL, na.rm = TRUE)
test[!is.na(test$YR_RMDL) & (test$YR_RMDL < 1750),"YR_RMDL"] <- median(test$YR_RMDL, na.rm = TRUE)

train[,"AYB"] <- sapply(train[,"AYB"], function(x) ifelse(is.na(x) || x < 1750, median(train[,"AYB"],na.rm = T),x))
validation[,"AYB"] <- sapply(validation[,"AYB"], function(x) ifelse(is.na(x) || x < 1750, median(validation[,"AYB"],na.rm = T),x))
test[,"AYB"] <- sapply(test[,"AYB"], function(x) ifelse(is.na(x) || x < 1750, median(test[,"AYB"],na.rm = T),x))

train[,"EYB"] <- sapply(train[,"EYB"], function(x) ifelse(x < 1900, median(train[,"EYB"],na.rm = T), x))
validation[,"EYB"] <- sapply(validation[,"EYB"], function(x) ifelse(x < 1750, median(validation[,"EYB"],na.rm = T), x))
test[,"EYB"] <- sapply(test[,"EYB"], function(x) ifelse(x < 1750, median(test[,"EYB"],na.rm = T), x))
```

In addition to the date columns, attributes for durations between these dates were tested and found to be useful. Note that it may result in negative numbers too.

* `AYB_TO_EYB` was calculated as `EYB` minus `AYB`
* `AYB_TO_SALEDATE` was calculated as `SALEDATE` minus `AYB`
* `AYB_TO_YR_RMDL` was calculated as `YR_RMDL` minus `AYB`
* `EYB_TO_YR_RMDL` was calculated as `YR_RMDL` minus `EYB`
* `EYB_TO_SALEDATE` was calculated as `SALEDATE` minus `EYB`
* `YR_RMDL_TO_SALEDATE` was calculated as `SALEDATE` minus `YR_RMDL`
 
```{r calc_time_periods}
train$AYB_TO_EYB <- (train[,"EYB"] - train[,"AYB"])
train$AYB_TO_YR_RMDL <- train[,"YR_RMDL"] - train[,"AYB"]
train$AYB_TO_SALEDATE <- year(train[,"SALEDATE"]) - train[,"AYB"]
train$EYB_TO_YR_RMDL <- train[,"YR_RMDL"] - train[,"EYB"]
train$EYB_TO_SALEDATE <- year(train[,"SALEDATE"]) - train[,"EYB"]
train$YR_RMDL_TO_SALEDATE <- year(train[,"SALEDATE"]) - train[,"YR_RMDL"]

validation$AYB_TO_EYB <- (validation[,"EYB"] - validation[,"AYB"])
validation$AYB_TO_YR_RMDL <- validation[,"YR_RMDL"] - validation[,"AYB"]
validation$AYB_TO_SALEDATE <- year(validation[,"SALEDATE"]) - validation[,"AYB"]
validation$EYB_TO_YR_RMDL <- validation[,"YR_RMDL"] - validation[,"EYB"]
validation$EYB_TO_SALEDATE <- year(validation[,"SALEDATE"]) - validation[,"EYB"]
validation$YR_RMDL_TO_SALEDATE <- year(validation[,"SALEDATE"]) - validation[,"YR_RMDL"]

test$AYB_TO_EYB <- (test[,"EYB"] - test[,"AYB"])
test$AYB_TO_YR_RMDL <- test[,"YR_RMDL"] - test[,"AYB"]
test$AYB_TO_SALEDATE <- year(test[,"SALEDATE"]) - test[,"AYB"]
test$EYB_TO_YR_RMDL <- test[,"YR_RMDL"] - test[,"EYB"]
test$EYB_TO_SALEDATE <- year(test[,"SALEDATE"]) - test[,"EYB"]
test$YR_RMDL_TO_SALEDATE <- year(test[,"SALEDATE"]) - test[,"YR_RMDL"]

#train[,"SALEDATE"] <- as.numeric(train[,"SALEDATE"])
#validation[,"SALEDATE"] <- as.numeric(validation[,"SALEDATE"])
#test[,"SALEDATE"] <- as.numeric(test[,"SALEDATE"])
```

The `YR_RMDL` attribute (and duration attributes that include the `YR_RMDL` attribute) were then converted to categorical attributes using the `optbin` function in the `BBMisc` package. This function finds bin (category) boundaries that maximise prediction of the target attribute (i.e., provide the highest information gain). The missing values were coded with the `NA` label. The corresponding attributes in the validation and test dataset were also categorised using the same bin boundaries as those in the training dataset.

Due to the large number of missing values, this method produced better accuracy (both accuracy and f-score) compared with raw values with NAs imputed with the median. A more complex strategy for binning (built decision trees that predicted `QUALIFIED` based on these attributes, and used node decision points to create bins) was attempted, but this produced no better results.

```{r categorise_dates}

set.seed(3433)
train[,"YR_RMDL"] <- (optbin(train[,c("YR_RMDL", "QUALIFIED")], na.omit = F))[,"YR_RMDL"]
#Levels: (1.88e+03,2e+03] (2e+03,2.02e+03] NA
validation[,"YR_RMDL"] <- sapply(validation[,"YR_RMDL"], function(x) factor(ifelse(is.na(x), "NA", ifelse(x<=2e+03,"(1.88e+03,2e+03]","(2e+03,2.02e+03]")), levels=levels(train$YR_RMDL)))
test[,"YR_RMDL"] <- sapply(test[,"YR_RMDL"], function(x) factor(ifelse(is.na(x), "NA", ifelse(x<=2e+03,"(1.88e+03,2e+03]","(2e+03,2.02e+03]")), levels=levels(train$YR_RMDL)))

train[,"YR_RMDL"] <- factor(train[,"YR_RMDL"], labels = c("A", "B", "C"))
validation[,"YR_RMDL"] <- factor(validation[,"YR_RMDL"], labels = c("A", "B", "C"))
test[,"YR_RMDL"] <- factor(test[,"YR_RMDL"], labels = c("A", "B", "C"))

set.seed(3433)
train[,"AYB_TO_YR_RMDL"] <- (optbin(train[,c("AYB_TO_YR_RMDL", "QUALIFIED")], na.omit = F))[,"AYB_TO_YR_RMDL"]
#Levels: (-68.3,72.5] (72.5,258] NA
validation[,"AYB_TO_YR_RMDL"] <- sapply(validation[,"AYB_TO_YR_RMDL"], function(x) factor(ifelse(is.na(x), "NA", ifelse(x<=72.5,"(-68.3,72.5]","(72.5,258]")), levels=levels(train$AYB_TO_YR_RMDL)))
test[,"AYB_TO_YR_RMDL"] <- sapply(test[,"AYB_TO_YR_RMDL"], function(x) factor(ifelse(is.na(x), "NA", ifelse(x<=72.5,"(-68.3,72.5]","(72.5,258]")), levels=levels(train$AYB_TO_YR_RMDL)))

train[,"AYB_TO_YR_RMDL"] <- factor(train[,"AYB_TO_YR_RMDL"], labels = c("A", "B", "C"))
validation[,"AYB_TO_YR_RMDL"] <- factor(validation[,"AYB_TO_YR_RMDL"], labels = c("A", "B", "C"))
test[,"AYB_TO_YR_RMDL"] <- factor(test[,"AYB_TO_YR_RMDL"], labels = c("A", "B", "C"))

set.seed(3433)
train[,"EYB_TO_YR_RMDL"] <- (optbin(train[,c("EYB_TO_YR_RMDL", "QUALIFIED")], na.omit = F))[,"EYB_TO_YR_RMDL"]
#Levels: (-84.2,30.8] (30.8,106] NA
validation[,"EYB_TO_YR_RMDL"] <- sapply(validation[,"EYB_TO_YR_RMDL"], function(x) factor(ifelse(is.na(x), "NA", ifelse(x<=30.8,"(-84.2,30.8]","(30.8,106]")), levels=levels(train$EYB_TO_YR_RMDL)))
test[,"EYB_TO_YR_RMDL"] <- sapply(test[,"EYB_TO_YR_RMDL"], function(x) factor(ifelse(is.na(x), "NA", ifelse(x<=30.8,"(-84.2,30.8]","(30.8,106]")), levels=levels(train$EYB_TO_YR_RMDL)))

train[,"EYB_TO_YR_RMDL"] <- factor(train[,"EYB_TO_YR_RMDL"], labels = c("A", "B", "C"))
validation[,"EYB_TO_YR_RMDL"] <- factor(validation[,"EYB_TO_YR_RMDL"], labels = c("A", "B", "C"))
test[,"EYB_TO_YR_RMDL"] <- factor(test[,"EYB_TO_YR_RMDL"], labels = c("A", "B", "C"))

set.seed(3433)
train[,"YR_RMDL_TO_SALEDATE"] <- (optbin(train[,c("YR_RMDL_TO_SALEDATE", "QUALIFIED")], na.omit = F))[,"YR_RMDL_TO_SALEDATE"]
#Levels: (-118,-14.6] (-14.6,134] NA
validation[,"YR_RMDL_TO_SALEDATE"] <- sapply(validation[,"YR_RMDL_TO_SALEDATE"], function(x) factor(ifelse(is.na(x), "NA", ifelse(x<=-14.6,"(-118,-14.6]","(-14.6,134]")), levels=levels(train$YR_RMDL_TO_SALEDATE)))
test[,"YR_RMDL_TO_SALEDATE"] <- sapply(test[,"YR_RMDL_TO_SALEDATE"], function(x) factor(ifelse(is.na(x), "NA", ifelse(x<=-14.6,"(-118,-14.6]","(-14.6,134]")), levels=levels(train$YR_RMDL_TO_SALEDATE)))

train[,"YR_RMDL_TO_SALEDATE"] <- factor(train[,"YR_RMDL_TO_SALEDATE"], labels = c("A", "B", "C"))
validation[,"YR_RMDL_TO_SALEDATE"] <- factor(validation[,"YR_RMDL_TO_SALEDATE"], labels = c("A", "B", "C"))
test[,"YR_RMDL_TO_SALEDATE"] <- factor(test[,"YR_RMDL_TO_SALEDATE"], labels = c("A", "B", "C"))
```

### Removing Highly Correlated Date Attributes

Date attributes were tested for pair-wise relatedness using `Goodman and Kruskalâ€™s tau` measure. The measure captures the degree of difference in one attribute that can be explained by another attribute. It is asymmetric (so the influence on x by y is not the same as y on x), which makes it useful in determining which attributes are redundant and can be dropped.

```{r check_correlated_date}
dt_attribs <- c('AYB', 'YR_RMDL', 'EYB', 'SALEDATE', 'AYB_TO_EYB', 'AYB_TO_SALEDATE', 'AYB_TO_YR_RMDL', 'EYB_TO_YR_RMDL', 'EYB_TO_SALEDATE', 'YR_RMDL_TO_SALEDATE')
dt_GKtauDF <- GKtauDataframe(train[,dt_attribs])
print(dt_GKtauDF)
```

!!! IMAGE !!!

Ignoring the main diagonal (correlation with self), the table shows that:
* `YR_RMDL` is a strong predictor of `YR_RMDL_TO_SALEDATE`
* `AYB_TO_YR_RMDL` is a strong predictor of `YR_RMDL` and `YR_RMDL_TO_SALE_DATE`
* `EYB_TO_YR_RMDL` is a strong predictor of `YR_RMDL` and `YR_RMDL_TO_SALE_DATE`

Therefore, the following attributes were removed from the analysis, as they are accounted for by other attributes: `YR_RMDL` and `YR_RMDL_TO_SALEDATE`. A validation on the reference model showed no drop in accuracy.

```{r drop_correlated_date}
train <- subset(train, select=-c(YR_RMDL, YR_RMDL_TO_SALEDATE))
validation <- subset(validation, select=-c(YR_RMDL, YR_RMDL_TO_SALEDATE))
test <- subset(test, select=-c(YR_RMDL, YR_RMDL_TO_SALEDATE))
```

### Inputing Missing values for Numerical attributes

The following attributes had between `20` and `50` examples with missing values: `BATHRM`, `HF_BATHRM`, `NUM_UNITS`, `ROOMS`, `BEDRM`, `STORIES`, `KITCHENS`, and `FIREPLACES`. As most examples with these missing values had `QUALIFIED` set to `0`, the missing values were replaced by the median (as opposed to mean to avoid the influence of outliers) of examples where `QUALIFIED` was `0`. The same transformation was applied to the validation and testing datasets too.

These numerical attributes had no missing values: `SALE_NUM`, `BLDG_NUM`, `PRICE`, `GBA`, `LANDAREA`.

```{r numeric_impute_missing}
numeric_with_na_attrs <- c("BATHRM", "HF_BATHRM", "NUM_UNITS", "ROOMS", "BEDRM", "STORIES", "KITCHENS", "FIREPLACES")

medians <- sapply(train[,numeric_with_na_attrs], function(x) median(x, na.rm = T))

for (attr in numeric_with_na_attrs) {
  train[is.na(train[,attr]),attr] <- medians[attr]
  validation[is.na(validation[,attr]),attr] <- medians[attr]
  test[is.na(test[,attr]),attr] <- medians[attr]
}

```

### Visualising Numerical attributes

The following numeric attributes were explored: `BATHRM`, `HF_BATHRM`, `NUM_UNITS`, `ROOMS`, `BEDRM`, `STORIES`, `SALE_NUM`, `BLDG_NUM`, `KITCHENS`, `FIREPLACES`, `PRICE`, `GBA`, `LANDAREA`.

Summary statistics (including number of missing values, mean, standard deviation, and quartiles - 0th, 25th, 50th, 75th and 100th percentiles) were reviewed.

```{r numeric_explore}
numeric_attrs <- c("BATHRM", "HF_BATHRM", "NUM_UNITS", "ROOMS", "BEDRM", "STORIES", "SALE_NUM", "BLDG_NUM", "KITCHENS", "FIREPLACES", "GBA", "LANDAREA")

skim_with(numeric = sfl(hist = NULL))(train[,c(numeric_attrs, "PRICE")])
```

!!! IMAGE !!!

It is clear that most of these attributes are skewed heavily. Various visualisations of each the numeric attributes were explored. An example of a box chart and frequency chart of `Land Area` and `ROOMS` is shown below. For visualisation, the raw attribute as well as the log (base 10) of the attribute were charted; the log was used as there was a lot of skewness in the data (i.e., there are very high numbers with most in a smaller range). These visualisations helped guide the missing value treatment and other transformations of these attributes, as outlined below.

```{r explore_plot_numeric, fig.cap='Sample Numeric Attribute exploration', fig.pos='h', fig.align='center'}
p1 <- ggplot(data = train, aes(x=QUALIFIED, y=LANDAREA)) + geom_boxplot(fill="steelblue") + labs(title="Land Area by Qualified", x="Qualified", y="Land Area in sq feet")
p2<- ggplot(train, aes(x=LANDAREA)) +  geom_density() + labs(title="Land Area by Frequency", x="Land Area", y="Frequency")
p3 <- ggplot(data = train, aes(x=QUALIFIED, y=log10(LANDAREA))) + geom_boxplot(fill="steelblue") + labs(title="Log(Land Area) by Qualified", x="Qualified", y="Log(Land Area in sq feet)")
p4<- ggplot(train, aes(x=log10(LANDAREA))) +  geom_density() + labs(title="Log(Land Area) by Frequency", x="Log(Land Area)", y="Frequency")
grid.arrange(p1, p2, p3, p4, nrow = 2)
```    

!!! IMAGE !!!

```{r explore_plot_numeric2, fig.cap='Sample Numeric Attribute exploration', fig.pos='h', fig.align='center'}
p1 <- ggplot(data = train, aes(x=QUALIFIED, y=ROOMS)) + geom_boxplot(fill="steelblue") + labs(title="# Rooms by Qualified", x="Qualified", y="# Rooms")
p2<- ggplot(train, aes(x=ROOMS)) +  geom_density() + labs(title="# Rooms by Frequency", x="# Rooms", y="Frequency")
p3 <- ggplot(data = train, aes(x=QUALIFIED, y=log10(ROOMS))) + geom_boxplot(fill="steelblue") + labs(title="Log(# Rooms) by Qualified", x="Qualified", y="Log(# Rooms)")
p4<- ggplot(train, aes(x=log10(ROOMS))) +  geom_density() + labs(title="Log (# Rooms) by Frequency", x="Log(# Rooms)", y="Frequency")
grid.arrange(p1, p2, p3, p4, nrow = 2)
```   

!!! IMAGE !!!

Based on this skew, the numerical attributes were converted to the logarithm (`base 10`) of these attributes (except `SALE_NUM` and `BLDG_NUM` - which didn't have the skew). Given that `log` (`base 10`) of `0` is negative infinity, these were replaced with `0`. However, the accuracy and f-score was tested on the reference model, and found to be slightly worse. Therefore, this transformation was not used.

```{r numerical_logvals}
numeric_attrs_skew <- c("BATHRM", "HF_BATHRM", "NUM_UNITS", "ROOMS", "BEDRM", "STORIES", "KITCHENS", "FIREPLACES", "GBA", "LANDAREA")

#train[,numeric_attrs_skew] = as.data.frame(lapply(train[,numeric_attrs_skew], function(x) ifelse(log10(x) == -Inf, 0, log10(x))))
#validation[,numeric_attrs_skew] = as.data.frame(lapply(validation[,numeric_attrs_skew], function(x) ifelse(log10(x) == -Inf, 0, log10(x))))
#test[,numeric_attrs_skew] = as.data.frame(lapply(test[,numeric_attrs_skew], function(x) ifelse(log10(x) == -Inf, 0, log10(x))))
```   

### Removing outliers in Numerical attributes

Consideration was given to removing outliers (for example `ROOMS` has one example with `101` rooms, with the second-highest being `39`). To test if a value was an outlier was to test if it was higher (or lower) than 3 standard deviations from the mean. Another method was also tried, where the value was an outlier if it was more than 1.5 times the inter-quartile range (75th minus 25th percentiles) higher than (or lower than) the 75th (or 25th) percentiles respectively.

However, in both cases, not only did removing the outliers make no difference (or in some cases perform worse) on the reference model, it is also possible that there are in fact properties with `100` rooms (as an example). With no more domain knowledge, it felt dangerous to remove them (for example, the testing data might have these), and so outliers were not removed.

### Handling ordered categorical attributes as numeric attributes

The `Condition` and `Grade` attributes came through as categorical, but were converted to numeric, so their "order" was captured. For example, `CONDITION` = `Excellent` (coded as `6`) is quantatively better than `CONDITION` = `Poor` (coded as `1`). They were already coded from worst to best (numbers increasing). They were then normalised (scaled) them to `0` to `1` to ensure comparable contribution with other numerical attributes. `33` of `35` missing values in the training dataset had a target `QUALIFIED` value `0`. Therefore, the missing values were replaced with the median of the training dataset where `QUALIFIED` = `0`.

```{r handle_cndtn_grade}
ord_cat_attrs <- c("CNDTN","GRADE")

most_freq_cats <- sapply(train[train$QUALIFIED=="no",ord_cat_attrs],function(x) which.max(table(x)))
names(most_freq_cats) <- ord_cat_attrs

blank_cat_example1 <- as.numeric(levels(factor(train[,ord_cat_attrs[1]]))[most_freq_cats[1]])
blank_cat_example2 <- as.numeric(levels(factor(train[,ord_cat_attrs[2]]))[most_freq_cats[2]])

train[is.na(train[,ord_cat_attrs[1]]),ord_cat_attrs[1]] <- blank_cat_example1
validation[is.na(validation[,ord_cat_attrs[1]]),ord_cat_attrs[1]] <- blank_cat_example1
test[is.na(test[,ord_cat_attrs[1]]),ord_cat_attrs[1]] <- blank_cat_example1

train[is.na(train[,ord_cat_attrs[2]]),ord_cat_attrs[2]] <- blank_cat_example2
validation[is.na(validation[,ord_cat_attrs[2]]),ord_cat_attrs[2]] <- blank_cat_example2
test[is.na(test[,ord_cat_attrs[2]]),ord_cat_attrs[2]] <- blank_cat_example2

train[,ord_cat_attrs] <- sapply(train[,ord_cat_attrs],function(x) BBmisc::normalize(as.numeric(x), method = "range", range=c(0,1)))
validation[,ord_cat_attrs] <- sapply(validation[,ord_cat_attrs],function(x) BBmisc::normalize(as.numeric(x), method = "range", range=c(0,1)))
test[,ord_cat_attrs] <- sapply(test[,ord_cat_attrs],function(x) BBmisc::normalize(as.numeric(x), method = "range", range=c(0,1)))
```

### Calculating Price per square foot

Given this is property data and we have the `PRICE` and `GBA` (Gross building area in square feet), it seemed useful to include the Price per square foot.

```{r price_per_foot}

train$PRICE_BY_GBA <- train$PRICE / train$GBA;
validation$PRICE_BY_GBA <- validation$PRICE / validation$GBA;
test$PRICE_BY_GBA <- test$PRICE / test$GBA;

```   

### Imputing the `PRICE` and `PRICE_BY_GBA` Attributes

The `Price` and `PRICE_BY_GBA` attributes had ~`12,134` missing values and ~`18,696` with `PRICE` values under `$1000` (mainly `$0`). Different approaches to imputing missing values were taken:

* Missing values were replaced with zero
* Missing values were replaced with the median
* The attribute was discretised (converted into categories/bins) using a decision tree. The decision tree provided the bin boundaries that maximised prediction of the target attribute

Of all these approaches, the last one provided the best accuracy & f-score, and was therefore used. This attribute seems to be a strong predictor of `QUALIFIED`, almost `88+%` accuracy just with this one attribute, hence the attention paid to various ways of treating this attribute.

```{r price_impute}

train[is.na(train$PRICE), "PRICE"] <- 0;
validation[is.na(validation$PRICE), "PRICE"] <- 0;
test[is.na(test$PRICE), "PRICE"] <- 0;

sub_model <- train(QUALIFIED~PRICE, 
                        data = train, 
                        method = "rpart",
                        #preProc = c("center", "scale"),
                        tuneLength=5,
                        trControl=trainControl(method = "cv", number = 5),
)

#sub_model$finalModel$splits[,4]
#64304.5 257992.0  82473.5 

train$PRICE <- sapply(train$PRICE, function(x) factor(ifelse(x<=64304.5,"A",ifelse(x<=82473.5,"B",ifelse(x<=257992.0,"C","D"))), levels=c("A", "B", "C", "D")))
validation$PRICE <- sapply(validation$PRICE, function(x) factor(ifelse(x<=64304.5,"A",ifelse(x<=82473.5,"B",ifelse(x<=257992.0,"C","D"))), levels=c("A", "B", "C", "D")))
test$PRICE <- sapply(test$PRICE, function(x) factor(ifelse(x<=64304.5,"A",ifelse(x<=82473.5,"B",ifelse(x<=257992.0,"C","D"))), levels=c("A", "B", "C", "D")))


train[is.na(train$PRICE_BY_GBA), "PRICE_BY_GBA"] <- 0;
validation[is.na(validation$PRICE_BY_GBA), "PRICE_BY_GBA"] <- 0;
test[is.na(test$PRICE_BY_GBA), "PRICE_BY_GBA"] <- 0;

sub_model <- train(QUALIFIED~PRICE_BY_GBA, 
                        data = train, 
                        method = "rpart",
                        #preProc = c("center", "scale"),
                        tuneLength=5,
                        trControl=trainControl(method = "cv", number = 10),
)

#sub_model$finalModel$splits[,4]
#57.03237, 171.34186, 235.28200

train$PRICE_BY_GBA <- sapply(train$PRICE_BY_GBA, function(x) factor(ifelse(x<=57.03237,"A",ifelse(x<=171.34186,"B",ifelse(x<=235.28200,"C","D"))), levels=c("A", "B", "C", "D")))
validation$PRICE_BY_GBA <- sapply(validation$PRICE_BY_GBA, function(x) factor(ifelse(x<=57.03237,"A",ifelse(x<=171.34186,"B",ifelse(x<=235.28200,"C","D"))), levels=c("A", "B", "C", "D")))
test$PRICE_BY_GBA <- sapply(test$PRICE_BY_GBA, function(x) factor(ifelse(x<=57.03237,"A",ifelse(x<=171.34186,"B",ifelse(x<=235.28200,"C","D"))), levels=c("A", "B", "C", "D")))

```

### Removing Highly Correlated Numeric Attributes

Numeric attributes were tested for pair-wise correlation (relatedness). When two attributes are highly correlated, including them adds little value (extra information) to the classification and in fact can cause un-necessary computation. The `findCorrelation` method was used to find pair-wise correlation of each numeric attribute in the training dataset which calculates the co-variance between them. A colour-coded visualisation of the correlation (darker means higher correlation, `blue` means negative, `red` means positive) helps visualise this.

```{r numeric_correlation}
full_numeric_attrs <- c("BATHRM", "HF_BATHRM", "NUM_UNITS", "ROOMS", "BEDRM", "STORIES", "SALE_NUM", "BLDG_NUM", "KITCHENS", "FIREPLACES", "GBA", "LANDAREA", "AYB", "EYB", "GRADE", "CNDTN", "USECODE",  "AYB_TO_EYB", "AYB_TO_SALEDATE", "EYB_TO_SALEDATE")

c <- cor(train[,full_numeric_attrs])
c_vars <- findCorrelation(c, cutoff = 0.8, names = TRUE, exact = ncol(c) < 100)

GGally::ggcorr(train, label_size=1)
```

!!! IMAGE !!!

Based on the `findCorrelation` function with an absolute threshold of `0.8` (highly correlated), the following attributes were dropped: `NUM_UNITS`, `AYB_TO_EYB`, `EYB_TO_SALEDATE`.

* `NUM_UNITS` was highly correlated with `INTWALL` and `FIREPLACES`
* `AYB_TO_SALEDATE` was highly correlated with `EYB_TO_SALEDATE`
* `AYB_TO_EYB` was highly correlated with `AYB`.

Attributes that predict the target attribute poorer (checked against the reference model) were dropped (`NUM_UNITS`, `AYB_TO_EYB` and `EYB_TO_SALEDATE`).

```{r drop_correlated_numeric}
train <- subset(train, select=-c(NUM_UNITS, AYB_TO_EYB, EYB_TO_SALEDATE))
validation <- subset(validation, select=-c(NUM_UNITS, AYB_TO_EYB, EYB_TO_SALEDATE))
test <- subset(test, select=-c(NUM_UNITS, AYB_TO_EYB, EYB_TO_SALEDATE))
```

### Missing values in Categorical attributes 

In addition to `PRICE`, `PRICE_BY_GBA`, `AYB_TO_YR_RMDL`, and `EYB_TO_YR_RMDL`, the following categorical attributes were found in the training dataset (excluding the target): `HEAT`, `AC`, `STYLE`, `STRUCT`, `EXTWALL`, `ROOF`, and `INTWALL`. Some of them came through as integers, but were converted to categorical attributes (`factors` in `R`). All of them had (the same) `20` examples with  missing values, and `19` of these had target `QUALIFIED` = `0`. 

While dropping them was considered, it was not possible to drop examples in the test set, so classifying them would have been a challenge. Therefore, the missing values were imputed with the most frequently occurring category corresponding to `QUALIFIED` = `0` in the training dataset for each of these attributes.

```{r handle_cat_attribs}
cat_attrs <- c("HEAT", "AC", "STYLE", "STRUCT", "EXTWALL", "ROOF", "INTWALL")

train[, cat_attrs] <- lapply(train[, cat_attrs], as.factor)
validation[, cat_attrs] <- lapply(validation[, cat_attrs], as.factor)
test[, cat_attrs] <- lapply(test[, cat_attrs], as.factor)

most_freq_cats <-sapply(train[train[,"QUALIFIED"]=="no",cat_attrs],function(x) which.max(table(x)))
names(most_freq_cats) <- cat_attrs

blank_cat_example <- train[is.na(train[,"HEAT"]),cat_attrs][1,]

for(i in 1:length(cat_attrs)) {
  blank_cat_example[i] <- levels(blank_cat_example[,i])[most_freq_cats[i]]
}

train[is.na(train[,cat_attrs[1]]),cat_attrs] <- blank_cat_example
validation[is.na(validation[,cat_attrs[1]]),cat_attrs] <- blank_cat_example
test[is.na(test[,cat_attrs[1]]),cat_attrs] <- blank_cat_example

```

The `USECODE` attribute came through as a number with `9` unique values, and is simply described as a `Property use code`, indicating that the attribute might need to be treated as categorical However, when it was converted to a categorical attribute (`factor` in `R`), even though the accuracy improved (on the reference model), the f-score was worse. Further, it took significantly longer to run, compared to when it was numeric. Therefore, it was left as a numeric attribute.

```{r handle_usecode}
#train[,"USECODE"] <- factor(train[,"USECODE"])
#validation[,"USECODE"] <- factor(validation[,"USECODE"])
#test[,"USECODE"] <- factor(test[,"USECODE"])
```

### Removing Highly Correlated Categorical Attributes

Categorical attributes were tested pair-wise for correlation (relatedness) using the `Cramer'r V` test (which calculates the level of association between categorical attributes between `0` and `1`, the higher, the more related). The `Cramer's phi` from the top 3 are shown:

* `EYB_TO_YR_RMDL` & `AYB_TO_YR_RMDL`: `0.7400530`
* `PRICE_BY_GBA` & `PRICE`: `0.7043760	`
* `AC` & `HEAT`: `0.5320308`

Using a threshold of `0.7` (which was reasonable based on documentation found), `AYB_TO_YR_RMDL` and `EYB_TO_YR_RMDL` were highly correlated, and `EYB_TO_YR_RMDL` was dropped (less predictive using the reference model). Similarly, `PRICE_BY_GBA` and `PRICE` were highly correlation, and `PRICE_PER_GBA` was dropped (less predictive using the reference model).

```{r drop_correlated_cat}

full_cat_attrs <- c("HEAT", "AC", "STYLE", "STRUCT", "EXTWALL", "ROOF", "INTWALL", "PRICE", "PRICE_BY_GBA", "AYB_TO_YR_RMDL", "EYB_TO_YR_RMDL")

attr_pairs <- expand.grid(attr1 = full_cat_attrs, attr2 = full_cat_attrs)

full_cram <- data.frame(attr.attr1 = character(), attr.attr2 = character(), cramer = numeric(), stringsAsFactors = FALSE)


for(i in 1 : nrow(attr_pairs)) {
  attr_pair <- attr_pairs[i,]
  if(attr_pair$attr1 != attr_pair$attr2) {
    xt <- xtabs(as.formula(paste("~",attr_pair$attr1, " + ", attr_pair$attr2)),data = train)
    cram <- cbind(attr=attr_pair, cramer=summary(assocstats(xt))$object$cramer)
    full_cram <- rbind(full_cram, cram)
  }
}

#head(full_cram[order(full_cram$cramer, decreasing = TRUE),],6)
train <- subset(train, select=-c(PRICE_BY_GBA, EYB_TO_YR_RMDL))
validation <- subset(validation, select=-c(PRICE_BY_GBA, EYB_TO_YR_RMDL))
test <- subset(test, select=-c(PRICE_BY_GBA, EYB_TO_YR_RMDL))

```

### One-hot-encode categorical attributes

One-hot-encoding is the splitting of a categorical attribute into multiple attributes, one for each factor level, with a `0` or `1` indicating whether that level was set for that attribute. The `dummyVars` function in `R` was used to convert categorical attributes to one-hot-encoded attributes.

The `9` categorical attributes resulted in `104` one-hot-encoded attributes since some categorical attributes had over `20` levels. This caused some algorithms (like Random Forest) to take a very long time (one run took over 24 hours with 10-fold cross validation). To reduce the number of attributes levels, the `nearZeroVar` in the `MASS` package was used to identify the one-hot-encoded categorical attributes that have a zero or near-zero variance (i.e., they stay "very" constant) - these are less likely to impact the classification outcome. In fact, dropping these attributes actually lead to an improvement in accuracy and f-score (when tested on the reference model), likely because it was less prone to over-fitting to poorly predictive attributes. It reduced the number of categorical attributes from `104` to `27`.

```{r cat_levels}
full_cat_attrs <- c("HEAT", "AC", "STYLE", "STRUCT", "EXTWALL", "ROOF", "INTWALL", "PRICE", "AYB_TO_YR_RMDL")

train_onehot <- predict(dummyVars(~., data=train[,full_cat_attrs]), train[,full_cat_attrs])
nzv_full <- nearZeroVar(train_onehot, saveMetrics = TRUE)
nzv_keep <- unique(c(row.names(nzv_full[nzv_full$nzv == FALSE,]), "PRICE.A", "PRICE.B", "PRICE.C", "PRICE.D"))
train_onehot <- as.data.frame(train_onehot[,nzv_keep])

train <- cbind(train_onehot, train[,!(names(train) %in% full_cat_attrs)])

validation_onehot <- predict(dummyVars(~., data=validation[,full_cat_attrs]), validation[,full_cat_attrs])
validation_onehot <- as.data.frame(validation_onehot[,nzv_keep])

validation <- cbind(validation_onehot, validation[,!(names(validation) %in% full_cat_attrs)])

test_onehot <- predict(dummyVars(~., data=test[,full_cat_attrs]), test[,full_cat_attrs])
test_onehot <- as.data.frame(test_onehot[,nzv_keep])

test <- cbind(test_onehot, test[,!(names(test) %in% full_cat_attrs)])
```

### Attribute Importance

Importance is a measure of the predictive power of the attribute on the target attribute. Having attributes that don't contribute meaningfully causes un-necessary computation. Given that some algorithms were very expensive to run (where the complexity is a function of the number of attributes), removing ones that are not needed was important.

Forward (and backward) attribute selection algorithms let you select an optimal set of attributes starting with none (or all) of the attributes and building (or culling) them in steps. Here, backward selection was used using the `glmStepAIC` function in the `MASS` package that builds an underlying `GLM` (generalised linear model) model and keeps culling attributes using `AIC` -  which is an estimate the relative information loss, as attributes are removed.

Another approach also taken, using the `Baruta` package in `R` to build 100 random forests and examine attributes used (and not used) to determine importance.

The top 3 most useful attributes in both cases were: `PRICE`, `GBA`, and `CNDTN`. It was re-assuting to see the level of agreement between the two approaches.

```{r importance}
train_control <-  trainControl(method = "none", verboseIter = TRUE)

step.model <- train(QUALIFIED ~ ., data = train,
                    method = "glmStepAIC", 
                    preProcess=c("scale","center"),
                    tuneLength = 1, 
                    trControl = train_control,
                    trace = 1
)

step.imp <- varImp(step.model$finalModel)
step.imp_full <- c(rownames(step.imp)[order(step.imp, decreasing = TRUE)], colnames(train[,!(colnames(train) %in% rownames(step.imp))]))[-47]
step.imp_full_tail <- tail(step.imp_full, 25)

#`PRICE.A`, `PRICE.D`, `CNDTN`, `GBA`, `PRICE.C`, `STRUCT.8`, `INTWALL.6`, `USECODE`, `BATHRM`, `STYLE.4`, `FIREPLACES`, `STRUCT.7`, `AYB_TO_SALEDATE`  `AYB`, `EXTWALL.14`, `SALEDATE`, `INTWALL.11`, `STYLE.6`, `INTWALL.3`, `AYB_TO_YR_RMDL.A` `STORIES`, `EYB`, `HF_BATHRM`, `AC.Y`, `AC.N`, `BEDRM`, `HEAT.1`, `HEAT.7`, `HEAT.13`, `STYLE.7`, `STRUCT.1`, `STRUCT.6`, `EXTWALL.22` `ROOF.1`, `ROOF.2`, `ROOF.6`, `ROOF.11`, `AYB_TO_YR_RMDL.B` `AYB_TO_YR_RMDL.C` `PRICE.B`, `ROOMS`, `SALE_NUM`, `BLDG_NUM`, `GRADE`, `KITCHENS`, `LANDAREA`.

importance_boruta <- Boruta(QUALIFIED ~ ., data=play.train, doTrace=2)  
full_importance_boruta <- TentativeRoughFix(importance_boruta)
baruta_imp <- imps[attStats(full_importance_boruta)$decision != 'Rejected', c('meanImp', 'decision')]
baruta_full <- imps2[order(-imps2$meanImp), ]
#plot(full_importance_boruta, cex.axis=.7, las=2, xlab="", main="Variable Importance")  
baruta_full_tail <- rownames(head(imps2[order(imps2$meanImp), ],25))
#baruta_full_tail <- c("STRUCT.6", "EXTWALL.22", "INTWALL.11", "STYLE.6", "INTWALL.3", "ROOF.2", "ROOF.11", "INTWALL.6", "HEAT.7", "STYLE.7", "HEAT.13", "ROOF.6", "AYB_TO_YR_RMDL.A", "STRUCT.7", "STRUCT.8", "HEAT.1", "AYB_TO_YR_RMDL.B", "EXTWALL.14", "HF_BATHRM", "ROOF.1", "STYLE.4","AC.Y","AC.N","STRUCT.1","SALE_NUM")

#`GBA`, `PRICE.A`, `EYB`, `CNDTN`, `PRICE.D`, `AYB`, `AYB_TO_SALEDATE`, `LANDAREA`, `SALEDATE`, `ROOMS`, `PRICE.C`, `GRADE`, `BEDRM`, `BATHRM`, `STORIES`, `FIREPLACES`, `PRICE.B`, `KITCHENS`, `USECODE`, `AYB_TO_YR_RMDL.C`, `SALE_NUM`, `STRUCT.1`, `AC.N`, `AC.Y`, `STYLE.4`, `ROOF.1`, `HF_BATHRM`, `EXTWALL.14`, `AYB_TO_YR_RMDL.B`, `HEAT.1`, `STRUCT.8`, `STRUCT.7`, `AYB_TO_YR_RMDL.A`, `ROOF.6`, `HEAT.13`, `STYLE.7`, `HEAT.7`, `INTWALL.6`, `ROOF.11`, `ROOF.2`, `INTWALL.3`, `STYLE.6`, `INTWALL.11`, `EXTWALL.22`, `STRUCT.6`.
```

The following `16` attributes were found to be in the bottom `25` of both methods (when attributes ordered by importance descending), and were therefore dropped: `HEAT.1`, `HEAT.7`, `HEAT.13`, `AC.N`, `AC.Y`, `STYLE.7`, `STRUCT.1`, `STRUCT.6`, `EXTWALL.22`, `ROOF.1`, `ROOF.2`, `ROOF.6`, `ROOF.11`, `AYB_TO_YR_RMDL.B` `HF_BATHRM`, `SALE_NUM`. Checks on the reference model suggested no noticable drop in performance (f-score).

```{r drop_unimportant}
unimportant_attrs <- intersect(step.imp_full_tail, baruta_full_tail)
#[1] "HEAT.1"           "HEAT.7"           "HEAT.13"          "AC.N"             "AC.Y"             "STYLE.7"          "STRUCT.1"         "STRUCT.6"         "EXTWALL.22"      
#[10] "ROOF.1"           "ROOF.2"           "ROOF.6"           "ROOF.11"          "AYB_TO_YR_RMDL.B" "HF_BATHRM"        "SALE_NUM"  

train <- train[,!names(train) %in% unimportant_attrs]
validation <- validation[,!names(validation) %in% unimportant_attrs]
test <- test[,!names(test) %in% unimportant_attrs]
```

```{r plot_important}
plot(as.raster(png::readPNG("C:\\Users\\d309144\\OneDrive - Telstra\\Microcredentials\\Advanced Analytics\\assignment\\importance.png")))
```

### Balancing the training dataset

In the training dataset, there were `29,090` examples with the target `QUALIFIED` attribute set to `1`, and `38,417` examples with target set to `0`. The dataset was therefore not fully balanced, it was skewed to value `0`. I applied the `SMOTE` (Synthetic Minority Over-Sampling Technique) over-sampling technique to the `1` examples in the training dataset to generate "synthetic" minority-class examples (i.e. `QUALIFIED`=`0`) so that the training dataset ended up with the same number of `0` and `1` values in the target attribute. I chose not to under-sample the majority class so as not to lose examples. The parameter for the number of nearest neighbours used to generate "likedness" was set to the default `5`.

```{r balance_dataset}
train <- DMwR::SMOTE(QUALIFIED~., train, perc.over=100)
```

The result was a balanced training dataset with the same number of examples with target class `0` and `1` (`38,417`). Having a balanced dataset makes it more reliable to interpret the accuracy measure.

# Model Build & Accuracy Testing

## Classification techniques used

The following classification techniques were used. Details of parameter settings are covered in each sub-section below.

* Decision Tree
* K-Nearest Neighbour
* Random Forest
- Gradient Boost (GBM)
- Support Vector Machines (SVM)
- Neural Network
- Ensemble (combination)

## Training Setup

### Accuracy Measures

The following measures were captured with a 10-fold cross-validation model build and prediction test on validation data:

* F1 (or f-score) - harmonic mean of the precision (proportion of positive predictions that were correct) and recall (proportion of positive examples that were correctly predicted), 1 implying perfect precision and recall.
* AUC (Area under the curve) - (0 to 1 - higher the better) is summary metric for the `ROC` curve and was reported. It captures the relation between true-positive rate and false positive rate.
* Time to run - elapsed time in hours (on my work laptop running `R` - single processor)

Note that without any further domain-specific understanding of the `QUALIFIED` target attribute, it was not clear whether to preference a false positive or false negative. However, given that the `kaggle` submission uses the `F1` metric, it was decided that the `F1` metric will be used. The `caret` package in `R`, uses accuracy by default, but allowed the use of a provided function to maximise. I coded the f-score (by calculating precision and recall and calculating the harmonic mean (twice the product divided by the sum)).

A combination of these measures were considered when determining the "best" model, as outlined in the sections below.

```{r accuracy_measure}

f1 <- function (data, lev = NULL, model = NULL) {
  precision <- posPredValue(data$pred, data$obs, positive = "yes")
  recall  <- sensitivity(data$pred, data$obs, postive = "yes")
  f1_val <- (2 * precision * recall) / (precision + recall)
  names(f1_val) <- c("F1")
  f1_val
} 
```

### Cross-validation

All models were build with 10-fold cross-validation (which is considered good practice). Here, the model training process is repeated 10 times, each time with 9/10 of the data used for training and the remaining 1/10 for validation. Every example therefore participates in the testing dataset (reducing the potential of over-fitting to a sample that the model happens to do well on). The combined accuracy is reported and is more reliable than running the model training once. This way, the chance of the algorithm getting "lucky" with the training dataset and not being able to generalise to an unknown dataset is reduced.

Classification algorithms in `R` (`caret` package) included a parameter called `trControl` that can be used to control model training, where these options were set. The `classProbs` option was also set to `TRUE` so class probabilities were captured in addition to predicted class - this allowed ROC curves to be drawn and ensemble models to be built more robustly (weighted by probability rather than the final class).

Further, the tuning parameter was set to `10`. This allowed 10 combination of hyper-parameters relevant to the model will be attempted and the best one automatically selected. The algorithm had default starting values for these.

```{r cross_validation}

train_control <-  trainControl(method = "cv", number = 10, verboseIter = TRUE, classProbs = TRUE, summaryFunction = f1)

my_train <- function (my_method = "rf") {
  train(QUALIFIED ~ .,
        data = train,
        method = my_method,
        trControl = train_control,
        preProcess = c("scale","center"),
        tuneLength = 10, 
        metric = "F1")
}

```

## Decision Tree Model

The `C5.0` algorithm is becoming the industry standard for decision trees, and so was used instead of the `ID3`. It grows a full decision tree (using information gain and entropy to determine splitting criteria) and then post-prunes (cull branches after trees are built) over-fitted branches of the tree. The `C50` package in `R` was used.

```{r model_dtree}

set.seed(3433)
model_c50 <- my_train(my_method = 'C5.0');
model <- model_c50
predict.raw <- predict(model, validation, type = "raw")
predict.prob <- predict(model, validation, type = "prob")
prediction <- prediction(predict.prob[[2]], ifelse(validation[,"QUALIFIED"] == "yes", 1, 0));

# Accuracy & f-score
confusionMatrix <- confusionMatrix(validation[,"QUALIFIED"], predict.raw, positive = "yes")
accuracy <- confusionMatrix[4]$byClass["Balanced Accuracy"]
fscore <- confusionMatrix[4]$byClass["F1"]

# ROC area under the curve
auc <- as.numeric(performance(prediction,"auc")@y.values)

# Summary of results
summ_stats <- as.data.frame(rbind(accuracy, fscore, auc))
colnames(summ_stats) <- "Measure"
summ_stats_c50 <- summ_stats
```

### Parameter Tuning

There are three tuning parameters available: `trials` (number of boosting iterations), `model` (one of `tree` - default, or `rules` if the tree should be decomposed into a rule-based model) and `winnow` (`FALSE` - default, or `TRUE` if predictor feature selection is to be used).

The `tuneLength` parameter in the `train` function in the `caret` package was set to `10`, meaning 10 different combinations of the above parameters were attempted before determining the one with the best performance.

The best fitting parameters were: `trials` = `80`, `model` = `rules`, `winnow` = `FALSE` on full training set.

### Performance

* Accuracy: `89.87%`
* f-score: `88.90%`
* AUC: `94.68%`
* Time to run: `~ 2` hours

```{r model_dtree_roc}
#plot(performance(prediction, "tpr", "fpr"), colorize=TRUE, lwd= 3, main= "ROC curve")
```

## K-Nearest Neighbour

The K-Nearest Neighbour algorithm classifies an example based on the most frequent target attributes of `k` examples closest to it, where distance is the Euclidean distance (square root of the sum of the square of the differences between each attribute). The `knn` package was used. Training was quick, but predictions took time, as it needed to calculate distances in the training dataset "on-the-fly".

```{r model_knn}

set.seed(3433)
model_knn <- my_train(my_method = 'knn');
model <- model_knn
predict.raw <- predict(model, validation, type = "raw")
predict.prob <- predict(model, validation, type = "prob")
prediction <- prediction(predict.prob[[2]], ifelse(validation[,"QUALIFIED"] == "yes", 1, 0));

# Accuracy & f-score
confusionMatrix <- confusionMatrix(validation[,"QUALIFIED"], predict.raw, positive = "yes")
accuracy <- confusionMatrix[4]$byClass["Balanced Accuracy"]
fscore <- confusionMatrix[4]$byClass["F1"]

# ROC area under the curve
auc <- as.numeric(performance(prediction,"auc")@y.values)

# Summary of results
summ_stats <- as.data.frame(rbind(accuracy, fscore, auc))
colnames(summ_stats) <- "Measure"
summ_stats_knn <- summ_stats
```

### Parameter Tuning

There is one tuning parameters available: `k` (number of nearest neighbours to consider).

The `tuneLength` parameter in the `train` function in the `caret` package was set to `10`, meaning 10 different combinations of this parameter were attempted before automatically determining the one with the best performance.

The best fitting parameters were: `k` = `5` on full training set.

### Performance

* Accuracy: `88.80%`
* f-score: `87.98%`
* AUC: `93.63%`
* Time to run: `< 20` minutes (prediction took longer as expected)

```{r model_knn_roc}
#plot(performance(prediction, "tpr", "fpr"), colorize=TRUE, lwd= 3, main= "ROC curve")
```

## Random Forest Model

The Random Forest algorithm is an example of bagging, where it uses an algorithm like decision trees to builds multiple deep trees in parallel (each has low bias but high variance) and them combines them to lower the variance. The `rf` package in `R` was used.

```{r model_rf}
set.seed(3433)
model_rf <- my_train(my_method = 'rf');
model <- model_rf
predict.raw <- predict(model, validation, type = "raw")
predict.prob <- predict(model, validation, type = "prob")
prediction <- prediction(predict.prob[[2]], ifelse(validation[,"QUALIFIED"] == "yes", 1, 0));

# Accuracy & f-score
confusionMatrix <- confusionMatrix(validation[,"QUALIFIED"], predict.raw, positive = "yes")
accuracy <- confusionMatrix[4]$byClass["Balanced Accuracy"]
fscore <- confusionMatrix[4]$byClass["F1"]

# ROC area under the curve
auc <- as.numeric(performance(prediction,"auc")@y.values)

# Summary of results
summ_stats <- as.data.frame(rbind(accuracy, fscore, auc))
colnames(summ_stats) <- "Measure"
summ_stats_rf <- summ_stats
```

### Parameter Tuning

The Random Forest model has the `mtry` parameter, which is the number of attributes randomly collected to be sampled at each split. The default is the square root of the number of attributes.

The `tuneLength` parameter in the `train` function in the `caret` package was set to `10`, meaning 10 different values of the `mtry` were attempted before determining the one with the best performance.

The best fitting parameters were: `mtry` = `83`.

### Accuracy

* Accuracy: `88.14%`
* f-score: `87.27%`
* AUC: `93.05%`
* Time to run: `~4-5` hours

```{r model_rf_roc}
#plot(performance(prediction, "tpr", "fpr"), colorize=TRUE, lwd= 3, main= "ROC curve")
```

## GBM (Gradient Boost Model)

The `GBM` algorithm is an example of a boosting algorithm where multiple trees are built sequentially, with subsequent trees focusing on examples that were previously mis-classified (by increasing their weights). The `gbm` package was used.

```{r model_gbm}

set.seed(3433)
model_gbm <- my_train(my_method = 'gbm');
model <- model_gbm
predict.raw <- predict(model, validation, type = "raw")
predict.prob <- predict(model, validation, type = "prob")
prediction <- prediction(predict.prob[[2]], ifelse(validation[,"QUALIFIED"] == "yes", 1, 0));

# Accuracy & f-score
confusionMatrix <- confusionMatrix(validation[,"QUALIFIED"], predict.raw, positive = "yes")
accuracy <- confusionMatrix[4]$byClass["Balanced Accuracy"]
fscore <- confusionMatrix[4]$byClass["F1"]

# ROC area under the curve
auc <- as.numeric(performance(prediction,"auc")@y.values)

# Summary of results
summ_stats <- as.data.frame(rbind(accuracy, fscore, auc))
colnames(summ_stats) <- "Measure"
summ_stats_gbm <- summ_stats
```

### Parameter Tuning

There are 4 tuning parameters available: `n.trees` (number of trees), `interaction.depth` (maximum nodes per tree), `shrinkage` (also known as the learning rate), `n.minobsinnode` (minimum number of observations in trees' terminal nodes).

The `tuneLength` parameter in the `train` function in the `caret` package was set to `10`, meaning 10 different combinations of this parameter were attempted before automatically determining the one with the best performance.

The best fitting parameters were: `n.trees` = `450`, `interaction.depth` = `10`, `shrinkage` = `0.1`, `n.minobsinnode` = `10` on full training set.

### Performance

* Accuracy: `90.83%`
* f-score: `90.18%`
* AUC: `96.04%`
* Time to run: `~ 3-4` hours

```{r model_gbm_roc}
#plot(performance(prediction, "tpr", "fpr"), colorize=TRUE, lwd= 3, main= "ROC curve")
```

## Support Vector Machine (SVM)

The SVM (Support Vector Machine) algorithm works by checking for  hyperplanes (higher dimension than attribute space) that can create good margins between classes of data by using `kernel` functions to transform the data. The `svmRadial` package was used; it uses a radial kernel function.

The numeric attributes were normalised for better performance, and only 50% of the examples in the training dataset (randomly drawn without replacement) were used, since training on the full dataset took too long.

```{r model_svm}
set.seed(3433)
model_svm <- my_train(my_method = 'svm');
train_sample <- train[sample(nrow(train), 0.50*nrow(train)),];

my_train <- function (my_method = "svm") {
  train(QUALIFIED ~ .,
        data = train_sample,
        method = my_method,
        trControl = train_control,
        preProcess = c("scale","center"),
        tuneLength = 10, 
        metric = "F1")
}


model <- model_svm
predict.raw <- predict(model, validation, type = "raw")
predict.prob <- predict(model, validation, type = "prob")
prediction <- prediction(predict.prob[[2]], ifelse(validation[,"QUALIFIED"] == "yes", 1, 0));

# Accuracy & f-score
confusionMatrix <- confusionMatrix(validation[,"QUALIFIED"], predict.raw, positive = "yes")
accuracy <- confusionMatrix[4]$byClass["Balanced Accuracy"]
fscore <- confusionMatrix[4]$byClass["F1"]

# ROC area under the curve
auc <- as.numeric(performance(prediction,"auc")@y.values)

# Summary of results
summ_stats <- as.data.frame(rbind(accuracy, fscore, auc))
colnames(summ_stats) <- "Measure"
summ_stats_svm <- summ_stats

```

### Parameter Tuning

The Radial SVM model has two parameters: `C` (penalty imposed on the model for making an error) and `sigma` (how dependent is the SVM boundary to just the closest points).

The `tuneLength` parameter in the `train` function in the `caret` package was set to `10`, meaning 10 different values of the `mtry` were attempted before determining the one with the best performance.

The best fitting parameters were: `sigma` = `0.0273`, `C` = `4`.

### Accuracy

* Accuracy: `89.72%`
* f-score: `88.89%`
* AUC: `93.40%`
* Time to run: `~8` hours

```{r model_svm_roc}
#plot(performance(prediction, "tpr", "fpr"), colorize=TRUE, lwd= 3, main= "ROC curve")
```

## Neural Networks

Neural networks are popular data mining algorithm (especially in the imaging space) and were roughly based on neurons, where examples "flow" through nodes and depending on where they land (i.e., error relative to the actual target), weights in the nodes are adjusted to "learn" from their mistake. This process is repeated for each example. The `nnet` package provides a feed-forward single-hidden-layer neural network algorithm, which was used. The numeric attributes were normalised for better performance.

```{r model_nnet}

set.seed(3433)
model_nnet <- my_train(my_method = 'nnet');
model <- model_nnet
predict.raw <- predict(model, validation, type = "raw")
predict.prob <- predict(model, validation, type = "prob")
prediction <- prediction(predict.prob[[2]], ifelse(validation[,"QUALIFIED"] == "yes", 1, 0));

# Accuracy & f-score
confusionMatrix <- confusionMatrix(validation[,"QUALIFIED"], predict.raw, positive = "yes")
accuracy <- confusionMatrix[4]$byClass["Balanced Accuracy"]
fscore <- confusionMatrix[4]$byClass["F1"]

# ROC area under the curve
auc <- as.numeric(performance(prediction,"auc")@y.values)

# Summary of results
summ_stats <- as.data.frame(rbind(accuracy, fscore, auc))
colnames(summ_stats) <- "Measure"
summ_stats_nnet <- summ_stats
```

### Parameter Tuning

There are two tuning parameters available: `size` (number of units in the hidden layer) and `decay` (regularisation parameter for weight decay used to avoid over-fitting).

The `tuneLength` parameter in the `train` function in the `caret` package was set to `10`, meaning 10 different combinations of this parameter were attempted before automatically determining the one with the best performance.

The best fitting parameters were: `size` = `9`, `decay` = `0.001` on full training set.

### Performance

* Accuracy: `89.87%`
* f-score: `89.00%`
* AUC: `94.68%`
* Time to run: `~ 4` hours

```{r model_nnet_roc}
#plot(performance(prediction, "tpr", "fpr"), colorize=TRUE, lwd= 3, main= "ROC curve")
```

## Ensemble

The Random Forest model (example of Bagging) and the GBM model (example of Boosting) are specialised ensemble models, that build and aggregate decision tree models in parallel and sequentially (respectively). They both performed well, and so a manual ensemble was attempted using all the models built (with the best hyper-parameter tuned for each).

The predictions (probabilities of class "yes") across all these models were used to calculate an average probability, and the target attribute was set to `yes` if it exceeded the threshold `0.5`, otherwise `no`. The average was also weighted based on the relative f-scores of the three models (i.e., better performing models were given more weight).

```{r ensemble}

predict.prob.ensemble_c50 <- predict(model_c50, validation, type = "prob")[[2]] 
predict.prob.ensemble_gbm <- predict(model_gbm, validation, type = "prob")[[2]] 
predict.prob.ensemble_nnet <- predict(model_nnet, validation, type = "prob")[[2]] 
predict.prob.ensemble_rf<- predict(model_rf, validation, type = "prob")[[2]] 
predict.prob.ensemble_knn <- predict(model_knn, validation, type = "prob")[[2]] 
predict.prob.ensemble_svm <- predict(model_svm, validation, type = "prob")[[2]] 

cut_off <- 0.5

wt_c50 <- 0.0
wt_gbm <- 1.0
wt_nnet <- 0.0
wt_rf <- 0.0
wt_knn <- 0.0
wt_svm <- 0.0

predict.prob <- ((predict.prob.ensemble_c50 * wt_c50) + (predict.prob.ensemble_gbm * wt_gbm) + (predict.prob.ensemble_nnet * wt_nnet) + (predict.prob.ensemble_rf * wt_rf) + (predict.prob.ensemble_knn * wt_knn) + (predict.prob.ensemble_svm * wt_svm))

predict.submit <- ifelse(predict.prob > cut_off, 1, 0)
predict.raw <- factor(ifelse(predict.prob > cut_off, 1, 0), labels=c("no", "yes"))

prediction <- prediction(predict.prob, ifelse(validation[,"QUALIFIED"] == "yes", 1, 0));

# Accuracy & f-score
confusionMatrix <- confusionMatrix(validation[,"QUALIFIED"], predict.raw, positive = "yes")
accuracy <- confusionMatrix[4]$byClass["Balanced Accuracy"]
fscore <- confusionMatrix[4]$byClass["F1"]

# ROC area under the curve
auc <- as.numeric(performance(prediction,"auc")@y.values)

# Summary of results
summ_stats <- as.data.frame(rbind(accuracy, fscore, auc))
colnames(summ_stats) <- "Measure"
summ_stats_ensemble <- summ_stats
summ_stats_ensemble
```

While numerous combinations of weights (adding up to `1`) were attempted to optimise the overall f-score, and various cut-offs (to determine when a prediction is a `yes`) were attempted, ultimately, the GBM algorithm out-performed all of them. Further, the best cut-off was found to be `0.5`.

# Selecting the best model

The selected model was the GBM model, with hyper-parameters: `n.trees` = `450`, `interaction.depth` = `10`, `shrinkage` = `0.1`, `n.minobsinnode` = `10` on full training set.

There were several considerations when selecting the best model:

* Accuracy (specifically, the f-score) - While training accuracy is good, it was important for the model to generalise well (i.e., not overfit - low variance). The GBM model reduces variance through boosting.
* The algorithm should also not be too simple (for example, using a linear regression model for a complex problem will result in high bias) and be robuse (deal with noise). The GBM algorithm does so.
* Speed / Time to run: This is a very important consideration given limited resources (i.e., not a very powerful laptop). It is the elapsed time taken during model build and prediction. For example, the `SVM` algorithm was promising, but took too long to run, and was therefore not preferred over the GBM algorithm.
* Interpretability: In some domains, it is critical to be able to explain the "rule set" behind the underlying model (for example, if used in determining if prisoners should get parole, you want to understand how it's using race, gender, etc.). In this case however, it seemed less important. Decision trees aid interpretability, while neural networks for example, don't. Associated with interpretability is the concept of goodness, where a simpler set of rules is preferred to a more substantive set even at the cost of some accuracy.

Based on all these considerations, the selected model was the GBM. Intuitively, this made sense because it seems like all the algorithms were able to classify ~88-90% of the cases fairly consistently, but struggled with the last 10-12%, and so a boosting algorithm that focused in on those mis-classified examples in subsequent iterations did better.

* It had the highest f-score
* It is much faster than SVM for example
* It is not interpretable, but in this domain, it may not matter as much.

The ROC curve for the best model (GBM)

```{r best_model}
plot(performance(prediction, "tpr", "fpr"), colorize=TRUE, lwd= 3, main= "ROC curve")
```

Confusion Matrix:

!!! FIX ME !!!

          Reference
Prediction   no  yes
       no  3703  565
       yes  114 3118
       
The model was mis-classifying `yes` more often than it was mis-classifying `no`, suggesting a higher specificity than sensitivity.

Other statistics:

* Accuracy: 90.95% 
* No Information Rate : 0.5089 (accuracy if random)
* P-Value [Acc > NIR] : < 2.2e-16 (value less than 0.05 suggests it significantly outperformed randomness)
* Sensitivity : 0.8466
* Specificity : 0.9701
* Pos Pred Value : 0.9647
* Neg Pred Value : 0.8676
* Prevalence : 0.4911
* Detection Rate : 0.4157
* Detection Prevalence : 0.4309

# Submitting Predictions

All the pre-processing steps applied on the training dataset was applied on the testing dataset and the best model was used to predict `QUALIFIED` attribute. the output was collated in the format needed by Kaggle and submitted successfully.

```{r submitting_predictions}

predict.prob.test_c50 <- predict(model_c50, test, type = "prob")[[2]] 
predict.prob.test_gbm <- predict(model_gbm, test, type = "prob")[[2]] 
predict.prob.test_nnet <- predict(model_nnet, test, type = "prob")[[2]] 
predict.prob.test_rf<- predict(model_rf, test, type = "prob")[[2]] 
predict.prob.test_knn <- predict(model_knn, test, type = "prob")[[2]] 
predict.prob.test_svm <- predict(model_svm, test, type = "prob")[[2]] 

# use cut_offs and wts from ensemble model (tuned to the best model)

predict.prob <- ((predict.prob.test_c50 * wt_c50) + (predict.prob.test_gbm * wt_gbm) + (predict.prob.test_nnet * wt_nnet) + (predict.prob.test_rf * wt_rf) + (predict.prob.test_knn * wt_knn) + (predict.prob.test_svm * wt_svm))

predict.submit <- ifelse(predict.prob > cut_off, 1, 0)

test_output <- as.data.frame(cbind("Row ID" = raw_test$row.ID, "Predict-Qualified" = predict.submit))
submission_num <- "03"
write.csv(test_output,paste0("C:\\Users\\d309144\\OneDrive - Telstra\\Microcredentials\\Advanced Analytics\\assignment\\submission_",submission_num,".csv"), row.names = FALSE, quote = FALSE)

```

# Reflections & Learnings

I've captured my reflections here, including what I've learnt, and what I would do differently if I did it again.

## Test and Learn Quickly

I spent a lot of time applying transformations to the data that I thought made sense. I initially completed all my pre-processing before testing, and was horrified to get a really poor accuracy score when tested against the first algorithm! I then decided to build a "reference" model that I would use to test each transformation (not once just once at the end) to see if it significantly improved performance (and also to test one type of transformation over another - like how to treat missing values).

In the future, I will definitely be taking the latter approach.

## It's science with art

As I researched different methods for pre-processing, setting parameters, etc., a lot of recommendations that I found on the internet were "it depends", suggesting that there is no "one-size-fits-all", and that experimentation is needed since every dataset is different. Therefore there is art as well as science in the process, and no "silver bullet" solution. With more experience, I suppose choices will become easier, but when starting, it just seems like there are so many possibly ways of approaching the problem, that one can get lost.

I also found it curious, that you can get to a decent accuracy fairly quickly, but making significant improvement from there was very difficult! In fact, you can quickly get bogged down by delving deep into a hypothesis to improve performance, and many many hours later, coming out with little or nothing to show for it. I learnt that the time would have been better spent on exploring more broadly.

## I under-estimated the effort

I did not expect algorithms to run for hours, with the longest one I had running for 24 hours on my laptop! The learning was to test my algorithm on subsets of data an prove it out before running it on a full 10-fold cross-validation across the entire dataset, with multiple hyper-parameters being tested at the same time!

One of my practical learnings was to consistently back-up my models and clearly document the parameters that I used to run them (apply version control). Initially, I'd get a good result, but I'd lose it and not be able to re-create it (or re-creating it would take another 24 hours!) I therefore started using `github` to back-up my work.

If I had more time (or was doing it again), I'd definitely do more work earlier, rather than procrastinating and trying to finish things quickly!

## Need more statistical background

While I can get away with running these algorithms, it's clear that a strong statistics background will help cement concepts better, and also provide a stronger basis for making one decision over another. I intend to pursue basic statistics courses so I can appreciate the use of commonly applied statistics (like `anova`).

## Ultimately, I learnt a lot

When I started this assignment, I didn't realise how much I will learn, from a brand new programming language (`R`), concepts in advanced analytics, to practical application through code. I explored tons of `R` packages to perform transformations and apply statistical methods and data mining algorithms. Because of the popularity of `R` among data scientists, I believe that this investment in learning `R` will pay off in my career.

The advanced analytics concepts in the class became much clearer when I applied them through this assignment. For example, the concept of bias and variance, when applied (for example through bagging - random forests), became a lot clearer, and I appreciate this trade-off a lot more now, than I might have otherwise.

In addition to going through the course notes and listening to all the lectures and workshops, doing the assignment also got me to research a lot on `Google.` `StackOverflow` was my best friend!